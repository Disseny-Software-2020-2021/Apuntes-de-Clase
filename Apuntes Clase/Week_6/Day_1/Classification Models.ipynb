{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "896fd6cc-4b80-406b-ace9-295fd526f1ad",
   "metadata": {},
   "source": [
    "# 6.1 Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ffdf41-0576-4c60-96ef-6c9f02e9e222",
   "metadata": {},
   "source": [
    "![blr](../../images/blr.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95e150e-e6b6-47fd-ae54-a5c84dae20d7",
   "metadata": {},
   "source": [
    "## 6.1.1 Binary Models. Churn Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2c9d84-2e7f-4ad5-9d2b-5d225101b4b0",
   "metadata": {},
   "source": [
    "Al hablar de modelos de clasificación binarios estamos hablando de modelos en los que nuestra variable dependiente es categórica y únicamente tiene dos clases a predecir, uno de los ejemplos más claros de este tipo de modelos son los problemas de 'Churn' o lo que es lo mismo la retención de clientes por parte de las compañías, y más que la retención es la detección temprana de si un cliente se va a ir de nuestra compañía o no. Suelen ser problemas complejos ya que la clase a predecir suele estar muy desbalanceada, ya que lo normal es que los clientes no se vayan de la compañía."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d8a969-fd82-47eb-b7b1-b68b986dfebf",
   "metadata": {},
   "source": [
    "![churn](../../images/churn.png)\n",
    "\n",
    "\n",
    "![churn2](../../images/churn2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffeda1-6d41-49b0-8d94-cdbe1d423afd",
   "metadata": {},
   "source": [
    "### **Regresión Logística**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f67e13-d048-4869-b41a-7865a2b262ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                # panel data, for handling dataframes\n",
    "pd.set_option('display.max_columns', None)         # show all columns of the dataframe\n",
    "\n",
    "import numpy as np                                 # numerical python, linear algebra library\n",
    "\n",
    "import pylab as plt                                # plotting library\n",
    "import seaborn as sns                              # plotting library\n",
    "sns.set(style='white')                             # seaborn style\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression            # logistic regression model   \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler                 # standarized\n",
    "from sklearn.preprocessing import LabelEncoder                 # Para codificar nuestra variable a predecir\n",
    "\n",
    "from sklearn.model_selection import train_test_split     # split data into train and test sets\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc776672-3436-426b-9811-59c4ea127c9d",
   "metadata": {},
   "source": [
    "Comenzaremos por explicar el modelo de regresión logisitica, modelo básico utilizado para clasificar, se basa en minimizar los pesos de la función sigmoide.\n",
    "\n",
    "![sig_plot](../../images/sig_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a8fe71-4532-4e10-ab4e-52f33123482f",
   "metadata": {},
   "source": [
    "Básicamente la función tratará de ajustarse para separar nuestros datos en dos grupos los que se encuentren a la izq de la función perteneceran a una clase, y los de la derecha a la otra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d8bc2-00f6-4530-ac5e-f2de6fdecf46",
   "metadata": {},
   "source": [
    "#### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de56617-f13f-4bab-b470-32f1e0aaddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../../../data/churn.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267ce35-8bd9-4dc9-896e-44e8803438f2",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60510f-bf99-44d2-8ff2-4e444e7ee065",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9232b7a7-6b58-44b3-910a-fdb0ce053253",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e13ea9c-771c-4545-8bcc-75f689b79169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Churn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20696ecf-7d4f-4b97-8c7c-a4738d78e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ChurnBinary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb744944-f167-403f-aa3f-a14d75d10bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.Churn.value_counts()/len(df)).plot.bar(color=['g', 'r'],    # plot customer churn rate\n",
    "                                           figsize=(10, 6),\n",
    "                                           title='Churn Rate',\n",
    "                                           rot=0,\n",
    "                                           fontsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226cf75b-d16f-4a68-a343-66b69631cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Churn.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad83e4f0-9383-4688-af8e-1e0ce519d9de",
   "metadata": {},
   "source": [
    "#### Transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14369b9e-6c17-4dff-9ba6-62b823fb8099",
   "metadata": {},
   "source": [
    "Tenemos varias opciones, podríamos tratar de crear un modelo únicamente con las variables numéricas que tenemos y ver cual es su desempeño, sería el modelo más simple y el que menos coste tendría, podríamos proponer alguna serie de transformaciones de las columnas categóricas y numéricas para poder realizar varios experimentos, pero para el caso que nos compete, realizaremos una transformación con label Enconder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd206a-c8e2-4bc4-9599-e4191f56b65d",
   "metadata": {},
   "source": [
    "##### Variables categóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0421696-ef6a-4bdf-9c94-4596ec74fe9d",
   "metadata": {},
   "source": [
    "**Label Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5e6a2-d8ec-49a0-84e3-df06370f4dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.PhoneService.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87c72d-0edb-4293-94ed-7502ea1f4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicto = {'Yes':1, 'No':0}\n",
    "\n",
    "prueba = df.PhoneService.apply(lambda x: dicto[x])\n",
    "prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d369b5-0581-4838-9ce1-8e033606b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2fc7bc-f394-4871-b8ff-81209f5a96cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_le = df.copy()\n",
    "for c in df_le.columns:\n",
    "    \n",
    "    if df_le.dtypes[c]==object and (c != 'customerID' or c != 'Churn'):\n",
    "        \n",
    "        le.fit(df_le[c].astype(str))\n",
    "        \n",
    "        df_le[c]=le.transform(df_le[c].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fce8a2-99d1-41a8-b3ee-8d3266697ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_le.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63717e-04af-4918-af1a-48b32d7b06cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b758bd2-b37f-4ad1-855b-bdcbf35a7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.MultipleLines.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6202902-2d85-490b-85cf-f6957ba06d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_le.MultipleLines.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aedd2f-79c3-4dab-add5-06a18423ef7f",
   "metadata": {},
   "source": [
    "##### Variables numéricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a546f-061a-49cf-adfa-cb1bb6cf3a55",
   "metadata": {},
   "source": [
    "Al igual que nos pasaba con las variables categóricas debemos de decidir que hacemos con las variables numéricas, debemos ver si estandarizamos los datos o no, para ello nos fijaremos en la descripción de nuestros datos estadísticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2351402-87e3-47c8-b180-a06584d8c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fdce13-7256-479e-8a91-28fec49808b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_le.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ffd65-f45c-4d00-8d06-4ba4682c6fdc",
   "metadata": {},
   "source": [
    "Como la mayoría de nuestros datos están entre 0 y 1 podemos usar el MinMaxScaler para transformar las columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600eaf60-96a8-4c15-82d0-f8a0e89e450f",
   "metadata": {},
   "source": [
    "**Antes de aplicar las transformaciones realizamos el train test split**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a68c4e-ab0b-4698-8e5f-80dfc91f0090",
   "metadata": {},
   "source": [
    "Al igual que en los modelos de regresión antes de realizar las transformaciones para estandarizar los datos haremos el train test split. En el caso de que tuvieramos varios experimentos deberíamos de hacerlel tts para cada uno de nuestros dataframes propuestos para realizar los experimentos. Y para cada uno de ellos deberíamos de entrenar un escaler diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76464200-fa76-451b-86c6-9e285e9bd81f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_le, X_test_le, y_train_le, y_test_le = train_test_split(df_le.drop(['customerID', 'Churn', 'ChurnBinary'], axis=1), df_le.Churn, random_state=42, test_size=.2, \n",
    "                                                    stratify=df_le.Churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083b2018-5722-4b8e-8fd2-903e2e1ccedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_le.value_counts(), y_test_le.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09aeac5-c253-4963-be9e-6a38ec8f1cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_le = MinMaxScaler().fit(X_train_le)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b256ab-41d0-4021-891b-53a4749c678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_le = mm_le.transform(X_train_le)\n",
    "X_test_le = mm_le.transform(X_test_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80694712-21e7-483f-a7d4-d994ce0f72ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_le).describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6b086-c476-4956-8c73-a03fdc0bf30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_le.value_counts(normalize=True), y_test_le.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d8c76-72d1-4b73-b2d2-631722d2b046",
   "metadata": {},
   "source": [
    "#### Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eea3066-32ea-4c32-87e5-315248457781",
   "metadata": {},
   "source": [
    "Lo primero que tenemos que hacer es inicializar nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ad4e8-f1f4-4910-9b78-773e7740132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_le = LogisticRegression(max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eff982-b120-494d-bc04-fc0999da9856",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_le.fit(X_train_le, y_train_le)\n",
    "pred = logreg_le.predict(X_train_le)\n",
    "score_train = logreg_le.score(X_train_le, y_train_le) #predicciones y evaluación sobre train\n",
    "score_test = logreg_le.score(X_test_le, y_test_le) #predicciones y evaluación sobre test\n",
    "        \n",
    "res_num = {'le_train_score': score_train,\n",
    "                   'le_test_score': score_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d7cbf-2923-4c8d-9390-86414e877841",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e835c70d-e92d-4640-a399-ff7141be758e",
   "metadata": {},
   "source": [
    "Parece ser que tenemos un buen resultado pero el accuracy en modelos de clasificación no es la mejor métrica de evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dff7b08-980e-44f8-b7be-c71f61a365f1",
   "metadata": {},
   "source": [
    "##### Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f02a11-f235-45d9-904b-e36ce25f2daf",
   "metadata": {},
   "source": [
    "**Matriz de confusión**\n",
    "\n",
    "![conf_matrix](../../images/conf_matrix.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fd633c-268f-474b-b78d-4d7a840afbe5",
   "metadata": {},
   "source": [
    "+ TP := True Positive (aciertos clase 1)\n",
    "+ TN := True Negative (aciertos clase 0)\n",
    "+ FP := False Positive (Error tipo I, decir 1 cuando es 0)\n",
    "+ FN := False Negative (Error tipo II, decir 0 cuando es 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476d3380-194f-43bf-a3d0-75dabeccd34e",
   "metadata": {},
   "source": [
    "+ Accuracy  := (TP+TN)/(TP+TN+FP+FN) (acierto)  ($\\frac{1}{n}\\sum 1(\\hat{y_i}=y_i$))\n",
    "+ Precision := TP/(TP+FP)\n",
    "+ Recall    := TP/(TP+FN)  (Sensibilidad, TPR)\n",
    "+ F1_Score  := 2·Recall·Precision/(Recall+Precision)\n",
    "\n",
    "(F1 funciona mejor que el accuracy cuando los datos no están balanceados y cuando FP y FN son muy diferentes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea6332-4308-48f8-87a5-cd38d1863602",
   "metadata": {},
   "source": [
    "![f1](../../images/f1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c9b941-cf58-4779-ab1c-2677ba14c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ab87a-743a-4be3-aec4-be47e6d8211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_le.fit(X_train_le, y_train_le)\n",
    "score_train = logreg_le.score(X_train_le, y_train_le)\n",
    "score_test = logreg_le.score(X_test_le, y_test_le)\n",
    "precision_train = precision_score(y_train_le, logreg_le.predict(X_train_le))\n",
    "precision_test = precision_score(y_test_le, logreg_le.predict(X_test_le))\n",
    "recall_train = recall_score(y_train_le, logreg_le.predict(X_train_le))\n",
    "recall_test = recall_score(y_test_le, logreg_le.predict(X_test_le))\n",
    "f1_train = f1_score(y_train_le, logreg_le.predict(X_train_le))\n",
    "f1_test = f1_score(y_test_le, logreg_le.predict(X_test_le))\n",
    "        \n",
    "res_num = {'le_train_score': score_train,\n",
    "           'le_test_score': score_test,\n",
    "           'le_train_precision': precision_train,\n",
    "           'le_test_precision': precision_test,\n",
    "           'le_train_recall': recall_train,\n",
    "           'le_test_recall': recall_test,\n",
    "           'le_f1_train': f1_train,\n",
    "           'le_f1_test': f1_test}\n",
    "        \n",
    "sns.heatmap(confusion_matrix(y_train_le, logreg_le.predict(X_train_le)), annot=True)\n",
    "plt.title('Confusion Matrix Train')\n",
    "plt.show();\n",
    "sns.heatmap(confusion_matrix(y_test_le, logreg_le.predict(X_test_le)), annot=True)\n",
    "plt.title('Confusion Matrix Test')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9953c89d-d864-41ba-b824-d8a9fd6577b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14e3e98-ee6e-4577-af97-159c456b39f8",
   "metadata": {},
   "source": [
    "Como vemos nuestro modelo no está funcionando muy correctamente, y esto es debido a que nuestros datos están muy desbalanceados, y nuestro modelo tiene un sesgo, básicamente está diciendo a todo que no, a parte está ligeramente overfitteado, esto podemos verlo en los resultados de train y test, ya que el f1 score en test es ligeramente superior que en train.\n",
    "Para tratar de compensar este tipo de incovenientes tenemos diferentes técnicas de balanceo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba4e12-8fdb-4ddf-9cd3-1b7e292d86b6",
   "metadata": {},
   "source": [
    "#### Balanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4f408-0f68-4a3b-aa56-9f20bce0f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df.Churn);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011363ec-9337-4d05-a1c5-6b5586568cc4",
   "metadata": {},
   "source": [
    "Cuando la variable objetivo está desbalanceada, tenemos varias opciones para tratar de balancear los datos:\n",
    "\n",
    "    - Métodos de undersampling\n",
    "    - Métodos de oversampling\n",
    " \n",
    "Elegiremos uno u otro método en base al número de datos de los que dispongamos, si disponemos un gran volumen de datos podemos decantarnos por técnicas de undersamplig en las que básicamente nos quedaremos con el mayor número de datos de la clase minoritaria, y tomaremos un sample de la misma longitud de la clase mayoritaria.\n",
    "\n",
    "Para los métodos de oversampling tenemos dos métodos:\n",
    "\n",
    "    - Random Oversampling: Con esta técnica el algoritmo genera datos nuevos de forma aleatoria basándose en los datos que le pasamos, hasta igualar ambas clases.\n",
    "    \n",
    "    - Smote: Este método es similar con la salvedad de que los datos generados de forma aleatoria se generan siguiendo una distribución normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edcc46a-8377-4287-b26b-3d48984dcfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9edd5-5f7a-4b4d-b086-2057a88d4cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a88309-7e36-4ae8-9db6-f5cae4197e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "rov = RandomOverSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f377706-15c2-41fb-b2a4-f56e0d3fe4bc",
   "metadata": {},
   "source": [
    "**Es importante tener en cuenta que solo aplicaremos la técnica de Oversampling o Undersampling a los datos de entrenamiento, los datos de test no se tocan.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a4c23-9d65-40e9-93b6-63f07b273148",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_le_sm, y_train_le_sm = smote.fit_resample(X_train_le, y_train_le)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd3486-e008-473f-a270-a1bf1890f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_le_sm.shape, y_train_le_sm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a5e86-d142-49fa-a43c-9a1da8538ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_le_sm.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bdb226-f9ff-44a9-add4-2eef0049297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y_train_le_sm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e8fca-d2ad-444a-ac62-ef67ceeae32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_le.fit(X_train_le_sm, y_train_le_sm)\n",
    "score_train_sm = logreg_le.score(X_train_le_sm, y_train_le_sm)\n",
    "score_test_sm = logreg_le.score(X_test_le, y_test_le)\n",
    "precision_train_sm = precision_score(y_train_le_sm, logreg_le.predict(X_train_le_sm))\n",
    "precision_test_sm = precision_score(y_test_le, logreg_le.predict(X_test_le))\n",
    "recall_train_sm = recall_score(y_train_le_sm, logreg_le.predict(X_train_le_sm))\n",
    "recall_test_sm = recall_score(y_test_le, logreg_le.predict(X_test_le))\n",
    "f1_train_sm = f1_score(y_train_le_sm, logreg_le.predict(X_train_le_sm))\n",
    "f1_test_sm = f1_score(y_test_le, logreg_le.predict(X_test_le))\n",
    "        \n",
    "res_sm = {'le_train_sm_score': score_train_sm,\n",
    "          'le_test_sm_score': score_test_sm,\n",
    "          'le_train_precision': precision_train_sm,\n",
    "          'le_test_precision': precision_test_sm,\n",
    "          'le_train_recall': recall_train_sm,\n",
    "          'le_test_recall': recall_test_sm,\n",
    "          'le_f1_train_sm': f1_train_sm,\n",
    "          'le_f1_test_sm': f1_test_sm}\n",
    "        \n",
    "sns.heatmap(confusion_matrix(y_train_le_sm, logreg_le.predict(X_train_le_sm)), annot=True);\n",
    "plt.title('Confusion Matrix Train')\n",
    "plt.show();\n",
    "sns.heatmap(confusion_matrix(y_test_le, logreg_le.predict(X_test_le)), annot=True);\n",
    "plt.title('Confusion Matrix Test')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06428b43-2579-4237-8de3-95264e567315",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cb4b57-860e-463a-aba5-990fbef229b1",
   "metadata": {},
   "source": [
    "#### Selección de Variables y reentreno de modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71efa13-ec95-4030-9434-c187f63f53d2",
   "metadata": {},
   "source": [
    "Como podemos oberservar el modelo ha mejorado un poco y se ha corregido el overfitting, llegados a este punto podemos valorar varias opciones o bien realizar un análisis más profundo de los datos y ver como están correlacionados nuestros datos en busca de colinealidad, realizar un estudio estudio de importancia de características, ajustar punto donde queremos valorar que un resultado sea Churn en la regresión logística, o cambiar de modelo.\n",
    "Antes de cambiar de modelo vamos a probar a ajustar el punto de intersección de la regresión logística y evaluar los coeficientes de cada una de las características y su correlación en busca de colinealidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a0a25-b8ea-4169-a235-e538ed011c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_le.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e260ca09-cb4c-47f3-a3f9-d9be00a2dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = dict(zip(list(df_le.drop(['customerID', 'Churn', 'ChurnBinary'], axis=1).columns),list(logreg_le.coef_[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc8b25-86f8-49a2-84c0-a51a0ae3d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b98705e-951e-4cd3-adb9-d1ec8629ad8e",
   "metadata": {},
   "source": [
    "Vamos a interpretar estos coeficientes, también denominados R statistic.\n",
    "\n",
    "Un valor positivo significa que al crecer la variable predictora, lo hace la probabilidad de que el evento ocurra. Un valor negativo implica que si la variable predictora decrece, la probabilidad de que el resultado ocurra disminuye. Si una variable tiene un valor pequeño de R entonces esta contribuye al modelo sólo una pequeña cantidad.\n",
    "\n",
    "De esto podemos extraer que si quitamos todas las columnas con un coeficiente negativo, nuestro modelo podría mejorar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63968571-1ed8-4367-939e-01e23ca269bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_coef = []\n",
    "\n",
    "for k,v in coefs.items():\n",
    "    if v < 0:\n",
    "        neg_coef.append(k)\n",
    "neg_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb94716-58b9-4051-8d43-d27faf631c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_le_pos_coef = df_le.drop(neg_coef+['customerID','ChurnBinary'], axis=1)\n",
    "\n",
    "df_le_pos_coef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f5ed5-735a-4de3-85eb-1aee8eba0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_le_pos_coef.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877ae45a-73ec-4487-be09-91496484bf9e",
   "metadata": {},
   "source": [
    "También comprobaremos la correlación de nuestas columnas restantes en busca de posible colinealidad entre ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721f6c30-0c78-4acd-bc3e-e824d65daf7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_heatmap_corr(data:pd.DataFrame, annot:bool=True, cmap:str=None, \n",
    "                       mask:bool=True, save:bool=False, title:str=None)->None:\n",
    "    \n",
    "    '''\n",
    "        Función que recibe un dataframe y devuelve la matriz de correlación en forma de mapa de color\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        \n",
    "        data: Dataset sobre el que queremos realizar la matriz de correlación\n",
    "        annot: Si queremos mostrar el valor de la correlación en la matriz, default = True\n",
    "        cmap: Paleta de colores que queremos usar para nuestro heatmap\n",
    "        mask: Parámetro para mostrar solo la triangular inferior de la matriz de correlación\n",
    "        save: Parámetro para salvar nuestro gráfico\n",
    "        title: Título que queremos que lleve nuestro gráfico\n",
    "    '''\n",
    "    \n",
    "    sns.set(style='white')     # estilo blanco hace que el fondo de la matriz sea transparente\n",
    "\n",
    "    if mask: # Si mask es True\n",
    "        mascara=np.triu(np.ones_like(data.corr(), dtype=bool))   # genera una mascara para tapar valores\n",
    "    else:\n",
    "        mascara = None # No aplicamos máscar\n",
    "\n",
    "    if cmap: # Si le hemos pasado una paleta de colores\n",
    "        c_map = sns.color_palette(cmap, as_cmap=True)\n",
    "    else:\n",
    "        c_map=sns.diverging_palette(0, 10, as_cmap=True)   # paleta de colores por defecto\n",
    "\n",
    "    plt.figure(figsize=(20,15))\n",
    "    p = sns.heatmap(data.corr(), # aplica el método corr() a nuestro dataset\n",
    "            mask=mascara, # aplica la mascara\n",
    "            cmap=c_map, # aplica la paleta de colores\n",
    "            vmax=1, # para establecer el valor máximo de valores\n",
    "            center=0, # establece el centro de la paleta de colores\n",
    "            square=True,\n",
    "            linewidth=0.5, # para aplicar borde a los cuadros de la matriz\n",
    "            cbar_kws={'shrink': 0.5}, # mostrar leyenda de colores\n",
    "            annot=annot # mostrar valores de la matriz\n",
    "           )\n",
    "    p.set_title(title, fontsize=20)\n",
    "    \n",
    "    if save:\n",
    "        try:\n",
    "            plt.savefig(f'graphics/{title}.png')\n",
    "        except:\n",
    "            destino = input('No exite la carpeta de destino, introduce un nombre para la carpeta de destino: ')\n",
    "            os.mkdir(destino)\n",
    "            plt.savefig(f'{destino}/{title}.png')\n",
    "    \n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e569491a-e6dc-482e-8ee3-fcef70e24185",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_heatmap_corr(df_le_pos_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff49ca9f-db1a-4f71-86e4-5a2af2ed8354",
   "metadata": {},
   "source": [
    "Vemos que la las variables independientes no tienen mucha correlación con nuestra variable dependiente, esto quiere decir que la solución al problema es compleja y hay que tratarla con cuidado, y también puede darnos una indicación de que los resultados que podemos esperar de los modelos no van ha ser muy buenos, pero tenemos que tratar de hacer todo lo posible para que estos sean lo más altos posibles.\n",
    "Respecto a la correlación entre las variables independientes vemos que salvo TotalCharges no hay excesiva colinealidad entre nuestras variables, por lo que nos quedaremos con ellas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e894e4-5c7b-44c5-a8ae-2999e66bbfd2",
   "metadata": {},
   "source": [
    "Como nuestro set de datos es diferente al original debemos de volver a realizar el train_test_split de nuevo, junto con el scaler y el smote para corregir el balanceo.\n",
    "\n",
    "Declararemos de nuevo nuestras X e y y aplicamos todo el proceso de transformaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3fc859-2dc9-4962-98a9-d574326d4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_le_pos_coef.drop('Churn', axis=1)\n",
    "y = df_le_pos_coef.Churn\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa56074a-89a6-42c7-8600-ff50a7eed45d",
   "metadata": {},
   "source": [
    "**Primero hacemos train test split de nuevo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b165343f-b6e5-482c-939d-2e23375e3800",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size=.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c2c870-c70f-49a7-aba7-9b0a098ff216",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd11201-5741-4c73-83b8-79c978f77b6c",
   "metadata": {},
   "source": [
    "**Seguidamente aplicamos el escalado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfc515e-3075-45ca-a508-c8609e443432",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MinMaxScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b174b76-739e-43ba-898e-99204e460e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mm.transform(X_train)\n",
    "X_test = mm.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d0cce2-b155-415a-ac21-c9dda3577e11",
   "metadata": {},
   "source": [
    "**Aplicamos SMOTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b10b81-0024-4e56-9c4a-b546159957da",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6da01a-8f67-4f2e-aa28-22466f340c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef27dc8-dfd0-46d1-8700-d1b276080c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fecc64-13ac-429d-8ac1-d3d4181eded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e1556-fe8f-43cd-a850-9068a639cd7f",
   "metadata": {},
   "source": [
    "**Inicializamos un modelo nuevo y lo entrenamos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf6abb3-b22b-499a-973a-9a91310074f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b888d42-6edd-4387-92e6-ec2c0d67d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d287a1d-40b1-415c-b3e5-597375453e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_train = lr.score(X_train, y_train)\n",
    "score_test = lr.score(X_test, y_test)\n",
    "precision_train = precision_score(y_train, lr.predict(X_train))\n",
    "precision_test = precision_score(y_test, lr.predict(X_test))\n",
    "recall_train = recall_score(y_train, lr.predict(X_train))\n",
    "recall_test = recall_score(y_test, lr.predict(X_test))\n",
    "f1_train = f1_score(y_train, lr.predict(X_train))\n",
    "f1_test = f1_score(y_test, lr.predict(X_test))\n",
    "        \n",
    "res = {'lr_train_score': score_train,\n",
    "       'lr_test_score': score_test,\n",
    "       'lr_train_precision': precision_train,\n",
    "       'lr_test_precision': precision_test,\n",
    "       'lr_train_recall': recall_train,\n",
    "       'lr_test_recall': recall_test,\n",
    "       'lr_f1_train': f1_train,\n",
    "       'lr_f1_test': f1_test}\n",
    "        \n",
    "sns.heatmap(confusion_matrix(y_train, lr.predict(X_train)), annot=True);\n",
    "plt.title('Confusion Matrix Train')\n",
    "plt.show();\n",
    "sns.heatmap(confusion_matrix(y_test, lr.predict(X_test)), annot=True);\n",
    "plt.title('Confusion Matrix Test')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e88887b-06f9-4c1b-aaa8-25d4723303b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d843cf-b521-4e29-8d7a-cb23fe826e5d",
   "metadata": {},
   "source": [
    "##### Ajuste del umbral de probabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ea5e5-898e-4e24-b6a3-307cb9649da1",
   "metadata": {},
   "source": [
    "Vemos que con este cambio hemos corregido un poco el tema del overfit pero los resultados del modelo en test siguen sin ser muy buenos, por lo que como último recurso antes de descartar el modelo y probar con otro vamos a tratar de ajustar el umbral de nuestra regresión a ver si con ello podemos mejorar sus f1_score, para ello en vez de usar el método predict, usaremos el método predict_proba, que lo que hará será devolvernos una lista con la probabilidad de que ocurra el evento de cada una de nuestras clases predictoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ba024-68e5-40e3-a69e-238770701ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = lr.predict_proba(X_train)\n",
    "probas[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254c39e-6a0c-4597-8404-87b9790ce751",
   "metadata": {},
   "source": [
    "Vamos a tratar de buscar un umbral óptimo para nuestra regresión, si tenemos en cuenta que un 26% de nuestros datos pertenecen a la clase 1 , es decir, Churn (clientes que se van), vamos a bajar el umbral en la clase para que de está manera el modelo no diga siempre que no a todo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f5d92-34b1-48fb-a0d5-ae0e7f54f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [1 if probas[i][1]>0.30 else 0 for i in range(len(probas))]\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574a5f88-4570-482a-b9fc-1a3223bcb2d6",
   "metadata": {},
   "source": [
    "Ya tenemos nuestras predicciones en base a nuestro nuevo umbral, ahora vamos a comprobar si nuestro modelo ha mejorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca357dae-ad85-4d44-b286-d01bbb77ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_proba = recall_score(y_train, preds)\n",
    "precision_proba = precision_score(y_train, preds)\n",
    "f1_score_proba = f1_score(y_train, preds)\n",
    "print(f'Recall: {recall_proba},\\nPrecision: {precision_proba},\\nf1: {f1_score_proba}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d20ae8-9b3f-4a67-9dd3-50d8b2dba3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_train, preds), annot=True)\n",
    "plt.title('Confusion Matrix Probas Train');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ef054-726e-4f7e-8563-90a912111637",
   "metadata": {
    "tags": []
   },
   "source": [
    "En train parece que ha mejorado un poco vamos a ver en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d793c7e0-3b56-41c1-a040-15551bf12db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_test = lr.predict_proba(X_test)\n",
    "preds_test = [1 if probas_test[i][1]>0.30 else 0 for i in range(len(probas_test))]\n",
    "recall_proba_test = recall_score(y_test, preds_test)\n",
    "precision_proba_test = precision_score(y_test, preds_test)\n",
    "f1_score_proba_test = f1_score(y_test, preds_test)\n",
    "print(f'Recall: {recall_proba_test},\\nPrecision: {precision_proba_test},\\nf1: {f1_score_proba_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc40a8-f079-4ac0-b84b-a741503ccd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, preds_test), annot=True)\n",
    "plt.title('Confusion Matrix Probas Test');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd963b0d-280d-479a-ab9a-93810036ef85",
   "metadata": {},
   "source": [
    "En test no hay mejora incluso el resultado empeora del f1 empeora pero el recall ha subido, y si nos fijamos en el error Tipo 2, es decir, estamos detectando con bastante acierto los posibles clientes potenciales a dejar la compañia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23b41b-d9da-455d-8204-c01b78fd79dd",
   "metadata": {},
   "source": [
    "### Otros modelos de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e472ced4-f3b9-4e07-ae55-abd710c6dd69",
   "metadata": {},
   "source": [
    "#### Decisión tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377fa03-c2a2-4349-998a-597caec5fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e929345-9488-4a3b-8b9c-a09b0bcbd3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_le.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fabb31-47d3-4c7d-96c3-721467b94f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_le.drop(['customerID','Churn','ChurnBinary'], axis=1)\n",
    "y = df_le.Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e47e9a-cb0d-4d54-b34e-94c38eeb1e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size=.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d515f-13ea-4017-b2b4-48775e0f9c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c5d2a5-f1d6-42f9-9166-40cf9a08cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51652ba-db7d-422c-9dc2-2f77509f7965",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = dt.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956a3a3a-57ac-43ec-a014-dcb11f2f315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_train, preds)\n",
    "recall = recall_score(y_train, preds)\n",
    "f1 = f1_score(y_train, preds)\n",
    "print(f'Recall: {recall},\\nPrecision: {precision},\\nf1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f63ca8-efa0-4915-b300-4baca0fab658",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_train, preds), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d54648c-01ae-4d4f-93c9-5817ba5815e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_test = dt.predict(X_test)\n",
    "precision_test = precision_score(y_test, preds_test)\n",
    "recall_test = recall_score(y_test, preds_test)\n",
    "f1_test = f1_score(y_test, preds_test)\n",
    "print(f'Recall: {recall_test},\\nPrecision: {precision_test},\\nf1: {f1_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8811c-6273-4ff7-b6c7-97b8a7940753",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, preds_test), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e89234f-24ae-4ce6-b291-03c3bd27a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(dt)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a739252e-6a19-4401-b8e8-32785304a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "feact = dict(zip(df_le.drop(['customerID','Churn','ChurnBinary'], axis=1).columns,dt.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d22a57b-48b7-4595-b3ad-0e95860edd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "plt.barh(pd.DataFrame(feact, index=[0]).T[0].index, \n",
    "         width=pd.DataFrame(feact, index=[0]).T[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cff42f-a5f4-4a76-9df7-269d5fd85f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_feact = []\n",
    "\n",
    "for k,v in feact.items():\n",
    "    if v > 0.03:\n",
    "        g_feact.append(k)\n",
    "g_feact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896e5ad-39d0-4c90-ad86-8200b43ef3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_le[g_feact]\n",
    "y = df_le.Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f1ffc-22ce-41f0-8f62-b72848203c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size=.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd43cd-b6b6-4383-8c09-2a048e0ef7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c6759-de2e-4521-b4d6-5827c9374635",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = dt.predict(X_train)\n",
    "precision = precision_score(y_train, preds)\n",
    "recall = recall_score(y_train, preds)\n",
    "f1 = f1_score(y_train, preds)\n",
    "print(f'Recall: {recall},\\nPrecision: {precision},\\nf1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c8ff4-f2b2-4e4c-8b00-931631662379",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_train, preds), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba67e2-4561-4b35-9d87-d5bb4e2f6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = dt.predict(X_test)\n",
    "precision_test = precision_score(y_test, preds_test)\n",
    "recall_test = recall_score(y_test, preds_test)\n",
    "f1_test = f1_score(y_test, preds_test)\n",
    "print(f'Recall: {recall_test},\\nPrecision: {precision_test},\\nf1: {f1_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20361530-f159-4008-840e-38e97f76c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, preds_test), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5fb043-f899-4257-8758-4c35c72440ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,70))\n",
    "plot_tree(dt)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1105810b-ce39-4934-812a-3404fa28da11",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac852dd-9b95-410b-9a93-e68786bd2a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c8dcad-e142-40e5-afa6-fb4f67a026cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_le.drop(['customerID','Churn','ChurnBinary'], axis=1)\n",
    "y = df_le.Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498399b-3bcd-4f0c-9846-a99c12f54cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size=.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8329de-6808-45b6-9ea3-55070eba75d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "smote = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a22e6-028e-45fa-8583-8e73bbe7e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4cffe5-cf1b-42f6-a22e-dce103e9ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0e0b4-0fc8-4872-8021-a95ce5165feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rf.predict(X_train)\n",
    "precision = precision_score(y_train, preds)\n",
    "recall = recall_score(y_train, preds)\n",
    "f1 = f1_score(y_train, preds)\n",
    "print(f'Recall: {recall},\\nPrecision: {precision},\\nf1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0500649-d215-48c7-ba3f-4c3f5b29b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_train, preds), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc424997-f308-46a7-9749-a0223f263a63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_test = rf.predict(X_test)\n",
    "precision_test = precision_score(y_test, preds_test)\n",
    "recall_test = recall_score(y_test, preds_test)\n",
    "f1_test = f1_score(y_test, preds_test)\n",
    "print(f'Recall: {recall_test},\\nPrecision: {precision_test},\\nf1: {f1_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d68bf2-be79-4cd2-9f7f-3da2ade922ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, preds_test), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf32c7-a48c-4376-b5ff-5de4e664404b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feact = dict(zip(df_le.drop(['customerID','Churn','ChurnBinary'], axis=1).columns,rf.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b42359-fc28-4b57-8f52-fb974a6756f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "plt.barh(pd.DataFrame(feact, index=[0]).T[0].index, \n",
    "         width=pd.DataFrame(feact, index=[0]).T[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05890547-b510-4c6a-b8a4-9f2f2f6914e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_feact = []\n",
    "\n",
    "for k,v in feact.items():\n",
    "    if v > 0.03:\n",
    "        g_feact.append(k)\n",
    "g_feact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e2bce-1b17-4403-bc19-cbec0c31cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_le[g_feact]\n",
    "y = df_le.Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b85eac-9c93-42e4-b3ec-5a42f004a1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size=.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43152f0f-aa4a-4fc7-8a18-dda1e2cababd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "smote = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69e5325-0c77-462b-96f7-e33635a5c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3708f9f-5728-43bd-bf1e-5a0fbaa0522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe046b6-3e18-49cd-a832-21f03cf69af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rf.predict(X_train)\n",
    "precision = precision_score(y_train, preds)\n",
    "recall = recall_score(y_train, preds)\n",
    "f1 = f1_score(y_train, preds)\n",
    "print(f'Recall: {recall},\\nPrecision: {precision},\\nf1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16c6f92-da38-4e4c-9e1c-6a1548d63805",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_train, preds), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41adfdd3-f550-434c-b143-9533b5df2418",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = rf.predict(X_test)\n",
    "precision_test = precision_score(y_test, preds_test)\n",
    "recall_test = recall_score(y_test, preds_test)\n",
    "f1_test = f1_score(y_test, preds_test)\n",
    "print(f'Recall: {recall_test},\\nPrecision: {precision_test},\\nf1: {f1_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc89c7-adbb-4e49-8e9d-b169c0001ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, preds_test), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176bff8-7ea7-4d9b-b9a0-67f4b8ac8dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = rf.predict_proba(X_train)\n",
    "preds = [1 if proba[i][1]>0.35 else 0 for i in range(len(proba))]\n",
    "precision = precision_score(y_train, preds)\n",
    "recall = recall_score(y_train, preds)\n",
    "f1 = f1_score(y_train, preds)\n",
    "print(f'Recall: {recall},\\nPrecision: {precision},\\nf1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb18c79-6d76-418e-abed-cf597b61396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_train, preds), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3fc24c-b8a3-4a5a-9154-7a46cd4e7a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = rf.predict_proba(X_test)\n",
    "preds = [1 if proba[i][1]>0.56 else 0 for i in range(len(proba))]\n",
    "precision = precision_score(y_test, preds)\n",
    "recall = recall_score(y_test, preds)\n",
    "f1 = f1_score(y_test, preds)\n",
    "print(f'Recall: {recall},\\nPrecision: {precision},\\nf1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ffb91-255e-41bf-b683-a4eed746c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, preds), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d9da46-1648-4e5b-9e7b-fb96e7304e85",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27febe7f-3914-406b-bd8e-c858313f517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57174efe-eb70-4567-8205-ca4a93fd61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRFClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8013e01b-56d0-4979-96d8-ee18ef1c01b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_le.drop(['customerID','Churn','ChurnBinary'], axis=1)\n",
    "y = df_le.Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0888b6-a86a-4883-9787-7d19fe09ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size=.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb635e6-3269-4131-a6c1-7069fc482401",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRFClassifier()\n",
    "smote = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19c340-6b54-413c-9590-be376edf03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b5a4a-6a5b-4c8f-ab1d-9cc1f57b65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff1b40-d416-4108-8f2f-a776a4990e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xgb.predict(X_train)\n",
    "precision = precision_score(y_train, preds)\n",
    "recall = recall_score(y_train, preds)\n",
    "f1 = f1_score(y_train, preds)\n",
    "print(f'Recall: {recall},\\nPrecision: {precision},\\nf1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3a6653-ac6c-4876-ab53-176fd837da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_train, preds), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb1e9ed-b86a-458e-917b-b95cf3c3089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = xgb.predict(X_test)\n",
    "precision_test = precision_score(y_test, preds_test)\n",
    "recall_test = recall_score(y_test, preds_test)\n",
    "f1_test = f1_score(y_test, preds_test)\n",
    "print(f'Recall: {recall_test},\\nPrecision: {precision_test},\\nf1: {f1_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3535758c-0a0b-409a-88ca-4749bad7bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, preds_test), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8a0b1-c848-442c-9485-e3b98f70bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7bef94-908e-4182-8a60-4ebb7eb472b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feact = dict(zip(df_le.drop(['customerID','Churn','ChurnBinary'], axis=1).columns,xgb.feature_importances_))\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.barh(pd.DataFrame(feact, index=[0]).T[0].index, \n",
    "         width=pd.DataFrame(feact, index=[0]).T[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ef32ed-76ab-4873-8b0c-bf2519574bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_feact = []\n",
    "\n",
    "for k,v in feact.items():\n",
    "    if v > 0.02:\n",
    "        g_feact.append(k)\n",
    "g_feact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde84171-01ac-4e3d-86fb-27a6d7b317a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_le[g_feact]\n",
    "y = df_le.Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b7757-217e-4ecd-ae5f-b45889ed9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42, test_size=.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477780a-6c8b-404c-a08a-bbcce670725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a241a739-45c3-45cb-b469-c3930787fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e5bb6b-f719-4adf-9943-46054759ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xgb.predict(X_train)\n",
    "precision = precision_score(y_train, preds)\n",
    "recall = recall_score(y_train, preds)\n",
    "f1 = f1_score(y_train, preds)\n",
    "print(f'Recall: {recall},\\nPrecision: {precision},\\nf1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42ecc0-2389-4089-908f-82082206bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_train, preds), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6f7af-11f3-4093-9a1f-2a56b7e21cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = xgb.predict(X_test)\n",
    "precision_test = precision_score(y_test, preds_test)\n",
    "recall_test = recall_score(y_test, preds_test)\n",
    "f1_test = f1_score(y_test, preds_test)\n",
    "print(f'Recall: {recall_test},\\nPrecision: {precision_test},\\nf1: {f1_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675a7fe-979a-46e2-818e-3513e17e4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, preds_test), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6065cd0c-16d1-42d4-87f7-8be330a4f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = xgb.predict_proba(X_test)\n",
    "preds = [1 if proba[i][1]>0.30 else 0 for i in range(len(proba))]\n",
    "precision = precision_score(y_test, preds)\n",
    "recall = recall_score(y_test, preds)\n",
    "f1 = f1_score(y_test, preds)\n",
    "print(f'Recall: {recall},\\nPrecision: {precision},\\nf1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1315bf3c-e222-4b7c-af54-c9897279440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, preds), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b4818-c319-4e61-a829-11966b001144",
   "metadata": {},
   "source": [
    "Para finalizar podemos corroborar que este tipo de problematica es compleja de solucionar y al final comprobamos que no por usar modelos más complejos vamos a obtener mejores resultados, en este caso el modelo con el que mejores resultados hemos obtenido de forma general ha sido la regresión logística ajustando el umbral de decisión, ya que disminuíamos el número de Falsos Negativos, y por lo tanto la fuga de clientes sin identificar es menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bfa82a-99bf-49a5-8268-1b7efeed1721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
